# Калькулятор степени интеграции информации (I₍интеграции₎(t)) на Python 3

---

## 1. Описание

Цель этого калькулятора — определить степень объединения информации в системе в момент времени *t*, используя меры энтропии и взаимной информации из теории информации.

---

### 2. Подход

- Меры энтропии:

  - Энтропия Шеннона (H): Измеряет неопределенность распределения вероятности случайной величины.

  - Взаимная информация (I): Количество информации, общей для двух случайных величин; измеряет уменьшение неопределенности одной случайной величины при знании другой.

- Алгоритм:

  - На каждом шаге времени *t*:

    - Собираем или получаем вероятностные распределения состояний системы.

    - Вычисляем энтропию для отдельных частей системы.

    - Вычисляем совместную энтропию и взаимную информацию между частями системы.

    - Определяем степень интеграции информации как функцию от взаимной информации.

---

### 3. Реализация на Python 3

[python-fiddle](https://python-fiddle.com/saved/os7STkI4au5zX6rgCvW6)


```python3
import numpy as np
from scipy.stats import entropy
import matplotlib.pyplot as plt

def shannon_entropy(prob_dist):
    """Вычисляет энтропию Шеннона для заданного распределения вероятностей."""
    return entropy(prob_dist, base=2)

def mutual_information(joint_prob_dist, marginal_prob_dist_X, marginal_prob_dist_Y):
    """Вычисляет взаимную информацию между двумя случайными величинами."""
    H_X = shannon_entropy(marginal_prob_dist_X)
    H_Y = shannon_entropy(marginal_prob_dist_Y)
    H_XY = shannon_entropy(joint_prob_dist.flatten())
    return H_X + H_Y - H_XY

def compute_I_integration(t, system_states):
    """
    Вычисляет степень интеграции информации I₍интеграции₎(t).
    :param t: момент времени
    :param system_states: список массивов состояний подсистем в момент времени t
    :return: значение I₍интеграции₎(t)
    """
    # Предполагаем, что system_states — это список из N массивов вероятностей состояний подсистем
    N = len(system_states)

    # Вычисляем энтропии отдельных подсистем
    H_subsystems = [shannon_entropy(states) for states in system_states]

    # Вычисляем совместное распределение вероятностей
    joint_prob_dist = system_states[0]
    for i in range(1, N):
        joint_prob_dist = np.outer(joint_prob_dist, system_states[i])

    # Вычисляем совместную энтропию
    H_joint = shannon_entropy(joint_prob_dist.flatten())

    # Вычисляем общую энтропию подсистем
    H_total = sum(H_subsystems)

    # Взаимная информация — разница между суммой энтропий подсистем и их совместной энтропией
    I_integration = H_total - H_joint

    return I_integration

# Пример использования

# Определяем моменты времени
time_points = np.linspace(0, 10, 100)

# Инициализируем список для хранения значений I₍интеграции₎(t)
I_integration_values = []

# Генерируем пример данных для двух подсистем
for t in time_points:
    # Пример: вероятности состояний зависят от времени
    p_subsystem1 = np.array([0.5 + 0.5 * np.sin(0.1 * t), 0.5 - 0.5 * np.sin(0.1 * t)])
    p_subsystem2 = np.array([0.5 + 0.5 * np.cos(0.1 * t), 0.5 - 0.5 * np.cos(0.1 * t)])

    # Нормализуем, чтобы суммы вероятностей были равны 1
    p_subsystem1 /= np.sum(p_subsystem1)
    p_subsystem2 /= np.sum(p_subsystem2)

    # Вычисляем I₍интеграции₎(t)
    I_t = compute_I_integration(t, [p_subsystem1, p_subsystem2])

    I_integration_values.append(I_t)

# Построение графика степени интеграции информации I₍интеграции₎(t)
plt.figure(figsize=(10, 6))
plt.plot(time_points, I_integration_values, label='I₍интеграции₎(t)', color='blue')
plt.title('Степень интеграции информации I₍интеграции₎(t)')
plt.xlabel('Время t')
plt.ylabel('I₍интеграции₎(t) (бит)')
plt.legend()
plt.grid(True)
plt.show()
```

---

### 4. Объяснение кода

1. Импорт библиотек:

   - numpy: Для работы с массивами и численными операциями.

   - scipy.stats.entropy: Для вычисления энтропии Шеннона.

   - matplotlib.pyplot: Для визуализации результатов.

2. Функция shannon_entropy(prob_dist):

   - Принимает массив вероятностей prob_dist.

   - Вычисляет энтропию Шеннона с основанием 2 (бит).

3. Функция mutual_information(joint_prob_dist, marginal_prob_dist_X, marginal_prob_dist_Y):

   - Вычисляет взаимную информацию между двумя случайными величинами.

   - Использует энтропии маргинальных и совместного распределений.

4. Функция compute_I_integration(t, system_states):

   - Принимает момент времени t и список состояний подсистем system_states.

   - Вычисляет энтропию каждой подсистемы.

   - Строит совместное распределение вероятностей всех подсистем.

   - Вычисляет совместную энтропию.

   - Степень интеграции информации I_integration определяется как разность между суммой энтропий подсистем и совместной энтропией:

I_(интеграции) = ∑ᵢ H_(подсистемаᵢ) - H_(совместная)


5. Пример использования:

   - Генерируем моменты времени от 0 до 10.

   - В цикле для каждого момента времени:

     - Определяем вероятности состояний двух подсистем, меняющиеся со временем.

     - Нормализуем вероятности.

     - Вычисляем I_integration.

     - Сохраняем значения для последующей визуализации.

6. Визуализация:

   - Строим график зависимости I₍интеграции₎(t) от времени t.

---

### 5. Пример вывода

График будет показывать, как степень интеграции информации между двумя подсистемами меняется во времени. Поскольку вероятности состояний обоих подсистем зависят от синусоидальных функций, степень интеграции информации также будет колебаться.


![как степень интеграции информации между двумя подсистемами меняется во времени](/Information-Integration-Degree-Calculator.png "как степень интеграции информации между двумя подсистемами меняется во времени")


---

### 6. Как адаптировать код под вашу систему

- Собственные данные:

  - Замените генерацию примерных вероятностей на ваши реальные данные.

  - Убедитесь, что вероятности корректно нормализованы.

- Большее количество подсистем:

  - Код легко расширяется на любое число подсистем.

  - Предоставьте список массивов вероятностей для каждой подсистемы в момент времени t.

- Изменение функций вероятностей:

  - Вероятности могут зависеть от более сложных функций времени или других параметров.

  - Обновите секцию, где генерируются p_subsystem1 и p_subsystem2.

---

### 7. Дополнительные замечания

- Корректность вероятностей:

  - Убедитесь, что все вероятностные распределения имеют суммы, равные 1, и не содержат отрицательных значений.

- Единицы измерения:

  - Энтропия вычисляется в битах, поскольку используем основание логарифма 2.

- Интерпретация результата:

  - Высокое значение I₍интеграции₎(t) указывает на высокую степень интеграции информации между подсистемами.

  - Нулевое значение означает, что подсистемы независимы в информационном смысле.

---

### 8. Заключение

Этот калькулятор предоставляет инструмент для количественной оценки степени интеграции информации в системе в любой момент времени *t*. Используя меры из теории информации, вы можете анализировать, как взаимодействуют различные части системы и как это взаимодействие меняется со временем. Это особенно полезно в нейронауках, теории информации и других областях, где важно понимать информационные связи между компонентами сложных систем.

---


Оглавление: 

- [Полный список алгоритмов для создания Python кода для расчёта представленных формул и теорий](/Computer-Science-and-Artificial-Intelligence/calc.md)
- [ЭИРО framework](/README.md)

