# Анализ

## КАНДИДАТЫ ДЛЯ ГИБРИДНОЙ МОДЕЛИ

### **Проблема (резюме)**

Моделирование адаптивной нервной системы требует учёта:
1. **Изменяемая размерность** — число степеней свободы меняется во времени
2. **Немарковская динамика** — зависимость от полной истории (path-dependence)
3. **Разрыв физика-семантика** — нет линейной связи между числом синапсов и информационной ёмкостью
4. **Открытость системы** — непрерывная генерация новых вариантов
5. **Многошкальность** — от миллисекунд до лет
6. **НЕОБРАТИМОСТЬ** — рассеяние энергии, невозможность возврата к исходному состоянию

---

## **ТОП-5 ФИНАЛЬНЫХ КАНДИДАТОВ ДЛЯ ГИБРИДА**

### **1. Thermodynamic Neural Network (TNN) ✓✓✓✓✓**

**Источник**: https://www.mdpi.com/1099-4300/22/3/256

**Ключевые свойства**:
- Узлы релаксируют глобально и **обратимо** (быстрая динамика)
- Рёбра релаксируют локально и **необратимо** (медленная диссипация)
- Многошкальная самоорганизация через консервацию заряда
- Обучение и инференс непрерывны (без разделения "learning"/"inference")
- Причинность встроена через "arrow of time" второго закона термодинамики
- **Нет градиентов** — термодинамическая релаксация
- **Нет метапараметров** типа learning rate

**Что не хватает**:
- Birth-death механизм (фиксированное число узлов/рёбер)
- Изменяемая размерность

**URL**: https://pmc.ncbi.nlm.nih.gov/articles/PMC7516712/
**ArXiv**: https://arxiv.org/abs/1906.01678

---

### **2. Growing Neural Cellular Automata (NCA) ✓✓✓✓✓**

**Источник**: https://distill.pub/2020/growing-ca/

**Ключевые свойства**:
- Морфогенез — рост из одной клетки в сложный паттерн
- **Локальные правила** → глобальная самоорганизация
- Регенерация после повреждений (экстремальная устойчивость)
- Дифференцируемая модель (градиентное обучение)
- Каждая клетка применяет одно и то же правило (единое "геномное" правило)
- Стохастическое обновление клеток

**Расширения**:
- **Conditional NCA**: одна модель генерирует разные паттерны по условию
- **Isotropic NCA**: инвариантность к вращению (нет внешнего ориентира)
- **Empowered NCA**: добавление empowerment как вторичного объектива

**Что не хватает**:
- Необратимость (нет термодинамической диссипации)
- Path-dependent память синапсов

**URL**: https://distill.pub/2020/growing-ca/
**GitHub**: упоминается в статье
**ArXiv (Isotropic)**: https://arxiv.org/abs/2205.01681

---

### **3. Open-Ended Evolution (OEE) Systems ✓✓✓✓**

**Источник**: https://alife.org/encyclopedia/introduction/open-ended-evolution/

**Ключевые свойства**:
- **Непрерывная генерация новизны** без финального состояния
- Неограниченно масштабируемая сложность
- Адаптивная новизна разных типов
- Evolutionary Activity Statistics для измерения
- **Tierra, Geb** — первые системы с открытой эволюцией

**Современные подходы**:
- **ASAL (Automated Search for Artificial Life)**: Vision-Language Models для поиска ALife симуляций
- Метрики: historical novelty, diversity, complexity growth
- Поиск симуляций с высокой новизной на каждом шаге

**Что добавляет**:
- Открытое пространство поиска (unbounded)
- Генерация качественно новых организмов

**URL**: https://sakana.ai/asal/
**Paper**: https://arxiv.org/pdf/2412.17799 (предполагаемый)

---

### **4. Synaptic Turnover + Structural Plasticity ✓✓✓✓**

**Источник**: https://pmc.ncbi.nlm.nih.gov/articles/PMC10245885/

**Ключевые свойства**:
- **Постоянное формирование и разрушение синапсов**
- Повышает скорость обучения и точность
- Эффективность максимальна при **дефиците ресурсов** (меньше параметров)
- Биологически реалистичные спайковые сети (SNN)
- Два типа пластичности:
  - **Functional** (изменение весов, быстро)
  - **Structural** (рождение/смерть синапсов, медленно)

**Биологические данные**:
- 50% синаптических изменений **независимы от активности**
- Turnover продолжается даже при блокировке пластичности
- Оптимальная компенсаторная пластичность ≤ величине флуктуаций

**Что добавляет**:
- Birth-death процессы на уровне синапсов
- Activity-dependent правила генерации/удаления

**URL**: https://pmc.ncbi.nlm.nih.gov/articles/PMC10245885/
**URL (теория)**: https://elifesciences.org/articles/62912

---

### **5. Dissipative Structures + Nonequilibrium Thermodynamics ✓✓✓✓**

**Источник**: https://www.nature.com/articles/s41467-025-67958-0

**Ключевые свойства**:
- Нелинейные вычисления **вне равновесия**
- Термодинамические нейроны (флуктуирующие степени свободы в тепловой ванне)
- **Производство энтропии** как тюнируемый ресурс
- Увеличение необратимости → улучшение генеративной производительности
- Диффузионные модели как обратный процесс разрушения структуры

**Теоретическая база (Prigogine)**:
- Спонтанное нарушение симметрии
- Дальнодействующие корреляции
- Формирование хаотических структур

**Применение к нейросетям**:
- Variational Onsager Neural Networks (VONNs)
- Минимизация диссипации в обучении
- Adaptive learning rates через термодинамические протоколы

**URL**: https://www.nature.com/articles/s41467-025-67958-0
**URL (diffusion)**: https://arxiv.org/abs/1503.03585

---

## **ОПТИМАЛЬНЫЙ ГИБРИД: Компоненты**

### **Архитектура: ITOMN (Irreversible Thermodynamic Open-Ended Morphogenetic Network)**

```
ITOMN = TNN (базовая термодинамика)
      + Growing NCA (морфогенетические правила)
      + Synaptic Turnover (birth-death механизм)
      + OEE (открытость и новизна)
      + Dissipative Dynamics (необратимость)
```

---

### **Компонент 1: Термодинамический скелет (TNN)**

**Узлы (нейроны)**:
- Быстрая обратимая релаксация мембранного потенциала
- Консервация "заряда" (активности)
- Болцмановское распределение состояний

**Рёбра (синапсы)**:
- Медленная необратимая диссипация
- Обновление только вблизи равновесия узлов
- **Производство энтропии** ∝ обучение

**Реализация**:
```python
# Pseudo-code
node_update: reversible, fast (Markov chain round-robin)
edge_update: irreversible, slow (only at node equilibrium)
entropy_production = sum(flux * force) > 0
```

---

### **Компонент 2: Морфогенетический рост (Growing NCA)**

**Локальные правила**:
- Каждый нейрон — "клетка" с внутренним состоянием
- Perception через соседей (3x3 kernel или граф)
- Update rule — единая нейронная сеть (~8K параметров)
- Residual updates (do-nothing initial behavior)

**Стохастическое обновление**:
- Не все клетки обновляются одновременно
- Dropout-подобный механизм

**Реализация**:
```python
# Perception
neighbors = conv2d(state, sobel_filters)
perception = concat([state, neighbors])

# Update
delta = update_network(perception)  # Neural net
new_state = state + delta * stochastic_mask
```

---

### **Компонент 3: Birth-Death механизм**

**Правила рождения синапсов**:
```python
P(birth) ∝ exp(-ΔG/kT) * novelty_measure * activity_correlation
```
- ΔG — свободная энергия конфигурации
- novelty — метрика новизны входа (CLIP embeddings или историческая дистанция)
- activity_correlation — корреляция активности пре- и пост-синаптического нейрона

**Правила смерти синапсов**:
```python
P(death) ∝ exp(age/τ) * (1 - activity) * resource_scarcity
```
- age — возраст синапса (монотонно растёт)
- activity — средняя активность за окно времени
- resource_scarcity — глобальный параметр (дефицит энергии/пространства)

**Регуляция**:
- Гомеостатический баланс: total_synapses ≈ target ± fluctuation
- Умеренная пролиферация (не слишком высокая, не слишком низкая)

---

### **Компонент 4: Открытость и новизна (OEE)**

**Метрики новизны**:
```python
# Historical novelty
novelty(t) = 1 - max_{t' < t} similarity(state(t), state(t'))
```
- Используется CLIP или другой foundation model
- Сравнение текущего состояния со всей историей
- Цель: максимизировать новизну на каждом шаге

**Evolutionary Activity Statistics**:
- Adaptive novelty — появление новых типов нейронов/синапсов
- Diversity — разнообразие конфигураций
- Complexity — размерность активного манифолда

**Архивирование**:
- Удачные конфигурации сохраняются в распределённой памяти
- Quality-Diversity алгоритмы (MAP-Elites)

---

### **Компонент 5: Необратимая диссипация**

**Производство энтропии на синапсе**:
```python
dS_ij/dt = (flux_ij * force_ij) > 0
flux = conductance * (V_pre - V_post)
force = gradient_of_potential
```

**Молекулярное состояние синапса**:
```python
Synapse_ij = {
  w_ij(t):     # вес (обратимо, минуты-часы)
  M_ij(t):     # молекулярный профиль (необратимо, дни-месяцы)
  A_ij(t):     # возраст (монотонно растёт)
  H_ij(path):  # история активаций (path functional)
}
```

**Динамика молекулярного состояния**:
```python
dM/dt = -γ * M + η(activity) + ξ(t)  # диссипация + обратная связь + шум
# γ — коэффициент диссипации (необратимость)
# η — activity-dependent изменения
# ξ — тепловой шум
```

---

### **Компонент 6: Path-dependent память**

**Path functional для синапса**:
```python
# Small Matrix Path Integral (SMatPI)
H_ij(t) = ∫[0,t] K(t-τ) * activity(τ) dτ
```
- K(t) — ядро памяти (немарковское, экспоненциально затухающее с несколькими масштабами)
- Вес синапса зависит от интеграла истории, а не только от текущей активности

**Эффективная память**:
```python
effective_capacity(synapse, age=20) >> effective_capacity(synapse, age=1)
# даже если physical_params одинаковы
```

---

## **МЕТРИКИ ИНФОРМАЦИОННОЙ ЁМКОСТИ (не биты!)**

1. **Entropy Rate**: скорость генерации информации системой
2. **Mutual Information**: I(input; behavior) — связь входа и поведения
3. **Manifold Dimensionality**: размерность латентного пространства представлений
4. **Fisher Information**: чувствительность к параметрам
5. **Memory Capacity**: для рекуррентных систем (Jaeger, 2001)
6. **Algorithmic Complexity**: способность генерировать новые структуры (open-endedness)

---

## **ПРЕИМУЩЕСТВА ГИБРИДА**

| Требование | TNN | NCA | Birth-Death | OEE | Dissipation | **Гибрид** |
|------------|-----|-----|-------------|-----|-------------|------------|
| Необратимость | ✓✓✓ | ✗ | ✗ | ✗ | ✓✓✓ | **✓✓✓** |
| Многошкальность | ✓✓✓ | ✓✓ | ✓✓ | ✓ | ✓✓✓ | **✓✓✓** |
| Изменяемая размерность | ✗ | ✓ | ✓✓✓ | ✓✓ | ✗ | **✓✓✓** |
| Path-dependence | ✓ | ✗ | ✓ | ✓ | ✓✓ | **✓✓✓** |
| Открытость | ✓✓ | ✓ | ✓ | ✓✓✓ | ✓ | **✓✓✓** |
| Морфогенез | ✗ | ✓✓✓ | ✓ | ✓✓ | ✗ | **✓✓✓** |

---

## **ВЫЗОВЫ РЕАЛИЗАЦИИ**

### **Математические**:
1. Формализация path functionals с переменным числом синапсов
2. Совмещение стохастических birth-death с детерминированной релаксацией TNN
3. Вычисление производства энтропии в дискретных спайковых сетях

### **Вычислительные**:
1. Эффективные алгоритмы для path integrals (использовать SMatPI локально, не глобально)
2. Параллелизация морфогенетических правил NCA
3. Adaptive time stepping для многошкальной динамики

### **Концептуальные**:
1. Определение "новизны" без предзаданных целей
2. Критерии успешности — не точность, а адаптивность
3. Измерение информационной ёмкости в незавершённой системе

---

## **КЛЮЧЕВЫЕ ПУБЛИКАЦИИ**

1. **TNN**: https://www.mdpi.com/1099-4300/22/3/256 (Hylton, 2020)
2. **Growing NCA**: https://distill.pub/2020/growing-ca/ (Mordvintsev et al., 2020)
3. **ASAL (OEE)**: https://sakana.ai/asal/ (Kumar et al., 2024)
4. **Synaptic Turnover**: https://pmc.ncbi.nlm.nih.gov/articles/PMC10245885/ (2023)
5. **Dissipative Dynamics**: https://www.nature.com/articles/s41467-025-67958-0 (2025)
6. **Diffusion Models**: https://arxiv.org/abs/1503.03585 (Sohl-Dickstein et al., 2015)

---

## **ВЫВОД**

**ITOMN** — единственный кандидат, объединяющий **все** требования:
- Необратимость (TNN + Dissipation)
- Изменяемая размерность (Birth-Death + NCA)
- Path-dependence (SMatPI + молекулярные состояния)
- Открытость (OEE + novelty search)
- Морфогенез (NCA + локальные правила)
- Многошкальность (разные временные масштабы компонентов)

Это **не про вычисления, а про адаптацию**.
