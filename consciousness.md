
# Emergent Integration and Recurrent Mapping Theory (EIRM)

| Abstract

Consciousness is one of the most enigmatic and complex phenomena, attracting the attention of scientists from various fields such as neuroscience, psychology, philosophy, and artificial intelligence. Despite significant progress in understanding the brain and cognitive processes, the nature of consciousness and the mechanisms of its emergence remain poorly understood.

This work presents the Emergent Integration and Recurrent Mapping Theory (EIRM), which offers an integrative approach to understanding consciousness. The theory is based on the concept of emergence, arguing that consciousness is an inherent property of complex neural systems, arising from the dynamic interaction of its components. Key elements of the theory include:



1. Information Integration: The brain integrates diverse sensory and cognitive information, creating a unified perception of the world.

2. Recurrent Processing: Recurrent neural networks and feedback loops play a critical role in maintaining and updating information necessary for consciousness.

3. Predictive Coding: The brain constantly generates predictions about incoming information, using prediction errors to adapt and learn.

4. Role of Attention and Working Memory: Mechanisms of attention and working memory modulate the processes of integration and recurrent processing, highlighting relevant information for conscious perception.

The EIRM theory aims to integrate existing models and data from neurobiological research into a unified conceptual framework. It offers an explanation of how complex neural processes lead to the emergence of subjective experience, and opens new perspectives for research in the field of consciousness.

| Introduction

1. The Problem of Consciousness in Modern Science

Consciousness is a central object of study in neuroscience, psychology, philosophy of mind, and related disciplines. Despite extensive research, the question of how physical processes in the brain give rise to subjective experience remains unanswered. This problem, known as the “hard problem of consciousness” in the words of philosopher David Chalmers, generates numerous discussions and controversies.


1.1. Existing Approaches to Studying Consciousness

   - Global Workspace Theory (GWT):
       - Proposes a model where consciousness arises when information becomes globally available to different cognitive systems.
       - Highlights the role of information dissemination throughout the brain.

   - Integrated Information Theory (IIT):
       - Argues that consciousness is linked to a quantifiable measure of integrated information in a system, denoted as Phi (Φ).
       - Focuses on the structure and complexity of informational connections in neural networks.

   - Predictive Coding:
       - Views the brain as a hierarchical system constantly predicting sensory inputs and updating its models based on prediction errors.
       - Emphasizes the role of learning and adaptation in perception.

   - Recurrent Processing:
       - Points to the importance of feedback loops and recurrent connections in the brain for maintaining and processing information.
       - Links consciousness to cyclical information processing.

1.2. The Need for an Integrative Theory

Existing models offer valuable insights into consciousness, but often consider it from a single perspective, overlooking the complexity of interactions within the brain. For example:

   - GWT emphasizes information dissemination but doesn’t sufficiently explain how this information is integrated and experienced consciously.
   - IIT provides a quantitative measure of consciousness but struggles to explain dynamic processes and the role of recurrence.
   - Predictive coding focuses on sensory processing and learning, but doesn’t encompass all aspects of subjective experience.

Therefore, there is a need for an integrative theory that combines key elements of these approaches, creating a holistic understanding of the mechanisms of consciousness.

## Core Principles of the EIRM Theory
### 1. Emergence of Consciousness

Definition of Emergence: Emergence is the appearance of new properties or qualities in a complex system that cannot be predicted or explained solely based on the properties of individual components.

#### Consciousness as an Emergent Property:

   - Consciousness is not the result of the activity of individual neurons or even individual brain areas.
   - It arises from the complex dynamics of interactions between different neural networks and systems.
   - The collective activity of neurons, their synchronization, and coordination lead to the emergence of subjective experience.

#### Examples of Emergence in Neural Networks:

   - Synchronization of neuronal oscillations: Collective waves of activity, such as gamma rhythms, are not inherent to individual neurons, but arise from their interactions.
   - Processing of images and concepts: High-level representations, such as concepts or abstract ideas, emerge from the interaction of multiple neural circuits.

### 2. Information Integration
#### The Necessity of Integration:

   - For a unified perception of the world, the brain must integrate information from various sensory channels and cognitive processes.
   - Integration allows disparate information to be connected, creating a comprehensive and coherent experience.

#### Integration Mechanisms

Functional connections between brain areas: Neural pathways connecting different regions, such as visual, auditory, and somatosensory cortices.

##### Global Workspace:


   - The concept of a global workspace suggests that consciousness arises when information becomes accessible to multiple cognitive processes through a widely distributed network.
   - Integrated information is broadcast throughout this network, allowing various cognitive systems to utilize it simultaneously.

#### Metric of Integrated Information:

Phi (Φ): A measure of the amount of integrated information in a system.

Phi-emergence (Φₑ):

Φₑ = ∫[](t₀)^(t₁) (( I_(integration)(t) × R_(recurrence)(t) )) dt

Here, I_(integration)(t) quantifies the degree of information integration at time t .


### 3. Recurrent Processing and Prediction
#### Role of Recurrent Connections:

   - Recurrent Networks: Networks where the output signals of neurons influence their own inputs or the inputs of previous layers.
   - Maintaining Activity: Recurrent connections allow information to be maintained in an active state, which is crucial for conscious perception and working memory.

#### Predictive Coding:

   - The Brain as a Predictor:
       - The brain continuously generates predictions about sensory inputs based on past experiences and internal models of the world.
       - Predictions allow for rapid and efficient processing of information by focusing on crucial changes.

   - Prediction Errors:
       - The difference between the prediction and the actual sensory input is called a prediction error.
       - Prediction errors are used to update internal models through learning and adaptation processes.

   - Bayesian Update:

P(θ | D) = P(D | θ) P(θ) / P(D)

P(θ | D) — the posterior probability of hypothesis θ after receiving data D .

> This approach models how the brain updates its predictions based on new data.



#### 4. The Role of Attention and Working Memory
##### Attention:

   - Selection Mechanisms: Attention allows for the selection of relevant information from a stream of sensory data for further processing.
   - Activity Modulation: Attention amplifies signals in relevant neural networks, increasing their contribution to conscious perception.

##### Working Memory:

   - Short-Term Information Retention: Working memory allows for the active maintenance of information for processing and decision-making.
   - Interaction with Recurrent Networks: Recurrent connections contribute to the sustained activity necessary for working memory.

#### 5. Dynamic Hierarchy of Neural Processes
##### Multi-level Processing:

   - The brain is organized hierarchically, from lower sensory areas to higher association regions.
   - Information is processed at different levels of complexity, from simple features to complex concepts.

##### Ascending and Descending Signals:

   - Ascending Pathways: Convey sensory information from receptors to higher areas.
   - Descending Pathways: Transmit predictions and modulate the activity of lower areas based on contextual information.

##### Role of Recurrence in the Hierarchy:

   - Recurrent connections link different levels, facilitating a continuous exchange of information and updating of predictions.


### Neurobiological Foundations of the Theory

The EIRM theory is supported by extensive neurobiological research data demonstrating the key role of information integration, recurrent connections, and predictive coding in brain function and consciousness.

![Neurobiological Foundations of the Theory](/1.jpg "Neurobiological Foundations of the Theory")


#### Integrative Neural Networks

   - Canonical Cortical Microcircuits:
       - The cerebral cortex contains repeating structures of neural circuitry that integrate inputs from various sources.
       - Pyramidal neurons: The primary excitatory cells of the cortex with extensive dendritic trees capable of combining numerous synaptic inputs.

   - Long-term and Short-term Connections:
       - Long-term associative connections: Link distant brain areas, such as the connection between the visual and auditory cortex.
       - Short-term local connections: Allow neurons within a single area to efficiently exchange information.



   - Functional Connectivity:
       - Neuroimaging studies show that conscious states are associated with increased functional connectivity between different brain regions.

#### Recurrent Connections and Loops

   - Thalamocortical Loops:
       - Thalamus: A central relay station for sensory information to the cortex.
       - Recurrent connections between the thalamus and cortex provide processing cycles that contribute to the awareness of sensory stimuli.

   - Corticocortical Feedback Loops:

   - Feedback from higher cognitive areas to sensory regions modulates the processing of incoming information.
       - This allows context and prior experience to influence perception.

   - Recurrent Networks in Visual Cortex:
       - V1 and higher visual areas: Recurrent connections between primary visual cortex (V1) and higher visual areas contribute to the processing of complex visual stimuli.

#### Predictive Coding

![Predictive Coding](/2.jpg "Predictive Coding")


   - Hierarchical Organization of the Brain:
       - Higher areas generate predictions about sensory inputs, which are passed down the hierarchy.
       - Lower areas compare these predictions to the actual input and transmit prediction errors upwards.

   - Neural Correlates of Prediction:
       - Neurophysiological studies show that the activity of some neurons corresponds to predictions, while others encode prediction errors.

   - Examples in Sensory Systems:

       - Visual System:
           - Illusions distort the brain’s predictions, demonstrating the role of predictive coding in perception.
           - Activity in visual cortex reflects expectations about visual stimuli.

       - Auditory System:
           - Predictions about sounds allow for faster and more accurate processing of speech and music.

#### The Role of Attention and Working Memory

   - Neural Mechanisms of Attention:
       - Pulvinar of the thalamus and frontal areas: Involved in attentional modulation by influencing the activity of sensory areas.
       - Neuromodulators: Such as acetylcholine and norepinephrine, regulate attentional levels and enhance the synchronization of neural networks.

   - Working Memory and Prefrontal Cortex:
       - The prefrontal cortex is responsible for maintaining information in working memory.
       - Recurrent connections within the prefrontal cortex and with other areas allow for the retention and manipulation of information.

   - Interaction with Other Systems:
       - Hippocampus: Interacts with working memory, contributing to consolidated memory and learning.
       - Striatum: Involved in action selection based on the stored information.

#### Evidence from Neuroimaging Studies

   - Functional MRI:
       - Conscious states show enhanced activity and connectivity in recurrent networks.
       - Performing tasks requiring attention and working memory is associated with increased activation of prefrontal and parietal areas.

   - EEG and MEG:
       - Synchronization of neuronal activity in the gamma range correlates with conscious perception and attention.
       - These oscillations reflect recurrent interactions between neural networks.

#### Clinical Observations and Pathologies

   - Disorders of Consciousness:
       - Patients with disorders of consciousness exhibit impaired functional connectivity and information integration.
       - Damage to areas responsible for recurrent connections results in diminished awareness.

   - Psychiatric Disorders:
       - Schizophrenia and other disorders may be associated with impairments in predictive coding and recurrent processing.
       - This leads to distorted perceptions and cognitive dysfunction.

   - Neurodegenerative Diseases:
       - Alzheimer’s disease and other dementias are characterized by impaired information integration and reduced functional connectivity.


#### Mathematical Modeling and Simulations

   - Models of Recurrent Neural Networks:
       - Computer models demonstrate how recurrent connections and predictive coding can give rise to complex cognitive phenomena.
       - These models help to understand the dynamics of neural networks and the mechanisms of emergence.

   - Learning and Adaptation:
       - The use of learning algorithms, such as Backpropagation Through Time, reflects the processes of updating internal models based on prediction errors.

The EIRM theory offers a holistic view of how consciousness might arise from complex neural processes of information integration, recurrent processing, and predictive coding. It integrates data from diverse areas of neuroscience, psychology, and computational neurobiology, offering a coherent and well-grounded model of consciousness.

---

### Mathematical Formalization

![Mathematical Formalization](/3.jpg "Mathematical Formalization")

#### Emergent Integrated Information (Φₑ)

▎Introduction

Emergent Integrated Information (Φₑ) is a central metric in the Emergent Integration and Recurrent Mapping Theory (EIRM). It is designed to quantify the level of consciousness in a system, considering both the amount of integrated information and the quality of its processing within the context of recurrent connections.


▎Metric Formula

Φₑ = ∫₍t₀₎^(t₁) [ I₍integration₎(t) × R₍recurrence₎(t) ] dt

where:

   - I₍integration₎(t) — the degree of information integration at time t.

   - R₍recurrence₎(t) — the degree of recurrent processing at time t.

   - The integral is calculated over the time interval from t₀ to t₁.




▎Metric Components

▎Degree of Information Integration (I₍integration₎(t))

Definition:

   - I₍integration₎(t) measures how interconnected and integrated information is across different parts of the system at time t.

   - A high degree of integration indicates that different components of the system exchange information effectively, creating a unified representation.

Methods of Calculation:

1. Entropic Approach:

- Uses mutual information between different components of the system.

- Formula:

I₍integration₎(t) = ∑₍i,j₎ [ H(Xᵢ(t)) + H(Xⱼ(t)) - H(Xᵢ(t), Xⱼ(t)) ]

where:
- H(Xᵢ(t)) — entropy of component i.

- H(Xᵢ(t), Xⱼ(t)) — joint entropy of components i and j.




2. Graph-based methods:

- Analyze connections between neurons or groups of neurons.

- Metrics such as clustering, centrality, and information transmission efficiency are used.

▎Degree of Recurrent Processing (R₍recurrence₎(t))




Definition:

- R₍recurrence₎(t) assesses the quantity and quality of feedback loops within the system.

- Recurrent connections allow the system to incorporate past experience when processing current information.

Methods of Calculation:

1. Density of recurrent connections:

- Formula:

- R₍recurrence₎(t) = (Number of recurrent connections at time t) / (Total possible recurrent connections)

2. Strength of recurrent connections:

- Considers the weight or effectiveness of each recurrent connection.

- Formula:

- R₍recurrence₎(t) = ∑₍i₎ ∑₍j₎ wᵢⱼ(t)

- where wᵢⱼ(t) is the weight of the recurrent connection between neurons i and j at time t.

3. Spectral analysis:

- Examines the dynamic properties of the network.

- Analyzing the eigenvalues and eigenvectors of the recurrent connection matrix to evaluate the system’s stability and dynamics.

▎Justification of the Φₑ Metric

   - Integration over time:

       - Allows for consideration of the dynamics of integration and recurrence processes.

       - Reflects the cumulative effect of interactions among system components.

   - Product of I₍integration₎(t) and R₍recurrence₎(t):

       - Indicates that a high level of consciousness is associated not only with information integration but also with active recurrent processing.

       - These two components mutually reinforce each other’s influence on the overall level of consciousness.




▎Advantages of the Φₑ Metric

   - Integrativeness:

       - Combines different aspects of neural dynamics.

       - Provides a more complete picture of the processes leading to consciousness.

   - Dynamism:
       - Accounts for changes over time, which is crucial for understanding consciousness as a process rather than a static state.

   - Quantitative assessment:

       - Enables comparisons between different states of the system.

       - Can be used for experiments and modeling.


▎Applications of the Φₑ Metric in Research

   - Comparing levels of consciousness:

       - Assessing Φₑ in different states (e.g., wakefulness, sleep, anesthesia).

       - Relating the metric to subjective reports of conscious experience.

   - Modeling pathologies:

       - Investigating impairments in integration or recurrence in various disorders (schizophrenia, dementia).

       - Developing diagnostic criteria based on Φₑ.

   - Artificial intelligence development:

       - Using the metric to evaluate the “consciousness” of artificial systems.

       - Optimizing the architecture of neural networks to increase Φₑ.

#### Recurrent Dynamical Systems

▎Neural Network State Equations

▎Introduction

Recurrent neural networks (RNNs) are a powerful tool for modeling systems where current outputs depend not only on current inputs but also on previous states. This reflects the brain’s characteristic ability to take past experience into account when processing new information.

▎Mathematical Model

The fundamental state equation is:

d𝐱/dt = 𝐟(𝐱(t), 𝐮(t), 𝑊)

where:

   - d𝐱/dt is the derivative of the network’s state with respect to time (rate of state change).

   - 𝐱(t) is the network’s state vector at time t.

   - 𝐮(t) is the vector of external input signals at time t.

   - 𝑊 is the weight matrix of connections within the network.

   - 𝐟 is a non-linear activation function (e.g., sigmoid, ReLU, hyperbolic tangent).

▎Model Components

▎Network State (𝐱(t))

   - Represents the activation values of all neurons in the network at time t.

   - Incorporates information from previous activations due to recurrent connections.

▎Input Signals (𝐮(t))

   - External stimuli entering the network.

   - Can be sensory data or signals from other parts of the system.



▎Weight Matrix (𝑊)

   - Contains:

       - 𝑊₍input₎ – weights of input connections (from inputs to hidden neurons).

       - 𝑊₍recurrence₎ – weights of recurrent connections (from neurons to themselves or other neurons within the network).

       - 𝑊₍output₎ – weights of output connections (from hidden neurons to network outputs).

   - Recurrent connections provide the system with memory and context.



▎Example: A Simple Recurrent Network

Neuron state update:

h(t) = ϕ(𝑊hh ⋅ h(t-1) + 𝑊hx ⋅ x(t) + bh)

where:

   - h(t) is the hidden layer state at time t.

   - h(t-1) is the hidden layer state at the previous time step.

   - x(t) is the input signal.

   - 𝑊hh is the recurrent weight matrix.

   - 𝑊hx is the input weight matrix.

   - bh is the bias vector.

   - ϕ is the activation function.

Network output:

y(t) = ψ(𝑊ho ⋅ h(t) + bo)

where:

   - y(t) is the network output.

   - 𝑊ho is the weight matrix from the hidden layer to the output.

   - bo is the bias vector.

   - ψ is the activation function (can vary depending on the task).



▎Advantages of Recurrent Systems

   - Handling temporal sequences:

       - Can process sequential data such as speech, text, and video.

       - Maintain internal memory of previous inputs.

   - Modeling non-linear dynamics:

       - Capable of representing complex dependencies in data.

       - Reflect the non-linear nature of neural processes in the brain.

▎Limitations and Problem Solving

   - Exploding and vanishing gradients:

       - When training long sequences, gradients can become too large or too small.

       - Solution: using advanced architectures such as LSTM or GRU.

  - Computational resource demands:

       - Modeling recurrent connections requires more computational power.

       - Optimization of algorithms and resource utilization.


▎Applications in Integrated Information Theory (IIT)

   - Modeling conscious processes:

       - Recurrent networks reflect the brain’s ability to integrate information over time.

       - Allow investigation into the dynamics of interactions between neuronal ensembles.

   - Analyzing activity patterns:

       - Investigating how specific activation patterns relate to subjective experience.

       - Potential for modeling different states of consciousness.


#### Predictive Updating of Models

▎Bayesian Updating

▎Introduction

In the context of Integrated Information Theory (IIT), predictive coding plays a crucial role. The brain continuously builds models of the surrounding world and updates them based on new data. Bayesian statistics provides a mathematical framework for such an updating process.



▎The Basic Bayesian Formula P(θ | D) = [ P(D | θ) × P(θ) ] / P(D)

where:

   - P(θ | D) is the posterior probability of model parameters θ after considering data D.

   - P(D | θ) is the likelihood of data D given parameters θ (how likely is it to obtain data D if the model with parameters θ is correct).

   - P(θ) is the prior probability of parameters θ before considering data D (our initial estimate of the parameters).

   - P(D) is the marginal likelihood of data D (a normalizing factor ensuring the sum of probabilities equals 1).

▎Components of Bayesian Updating

   - Prior distribution P(θ):

       - Represents initial beliefs about model parameters.

       - In the brain, it may correspond to previously acquired knowledge or intuition.

   - Likelihood P(D | θ):

       - Evaluates how well the current model explains new data.

       - Reflects the brain’s expectations regarding sensory inputs.

   - Posterior distribution P(θ | D):

       - Updated beliefs after considering new data.

       - Allows for adjusting the model for more accurate prediction of future events.

   - Marginal likelihood P(D):

       - Calculated as an integral over all possible values of θ.

       - Often difficult to compute, but a constant when updating parameters.




▎Application in Predictive Coding

   - Prediction error:

       - The brain compares expected sensory signals with actual ones.

       - The difference between them is used to update the model.

   - Updating model parameters:

       - Parameter adjustments are made in the direction of reducing prediction error.

       - The greater the mismatch, the more significant the parameter change.


▎Mathematical Representation of Updating

   - Parameter update:

   - θn+1 = θn + η × ∇θ L(θn, Dn)

   - where:

       - θn is the current model parameters at step n.

       - η is the learning rate.

       - ∇θ L(θn, Dn) is the gradient of the loss function L with respect to parameters θ on data Dn.

   - The loss function L can be defined as the negative log-likelihood:

   - L(θ, D) = -ln P(D | θ)




▎Applications in the Brain

   - Sensory systems:

       - Predicting sensory inputs based on context and past experience.

       - For example, anticipating a certain visual stimulus in a familiar setting.

   - Attention and learning:

       - Increased attention to stimuli that do not match expectations.

       - Accelerated learning on anomalous or novel data.

▎Example

Imagine a person expecting to see a green traffic light when approaching an intersection (prior knowledge). However, the light turns out to be red (new data). The prediction error leads to a model update – the person now adjusts their expectations and stops.

▎Significance in IIT

   - Adaptability:

       - Ability to respond quickly to changes in the environment.

       - Constant updating of internal models for more accurate interaction with the world.

   - Emergence of consciousness:

       - Predictive coding and Bayesian updating contribute to the formation of subjective experience.

       - Allow integration of past experience with current stimuli, creating a continuous stream of consciousness.

#### Conclusion

The detailed aspects of the integrated information metric, recurrent dynamical systems, and predictive updating of models highlight the complexity and multi-faceted nature of the processes underlying consciousness. The Φe metric integrates quantitative measurements of integration and recurrence, providing a tool for assessing the level of consciousness. Recurrent neural networks model the dynamics of neuronal interactions, reflecting the brain’s ability to account for past experience. Bayesian updating describes how the brain updates its internal models based on new data, which is crucial for adaptability and learning.

IIT, by combining these components, offers a comprehensive view of the mechanisms of consciousness, opening new avenues for research in neuroscience and related fields.

---















